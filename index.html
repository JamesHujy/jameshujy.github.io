<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jinyi Hu</title>
  
  <meta name="author" content="Jinyi Hu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jinyi (James) Hu</name>
              </p>
              <p>I am a fourth-year Ph.D. student at Tsinghua, <a href="https://nlp.csai.tsinghua.edu.cn/">THUNLP</a> lab, advised by Prof. <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>. I'm currently a visit student at <a href="https://hanlab.mit.edu/">MIT Han Lab</a>, advised by Prof. <a href="https://hanlab.mit.edu/songhan">Song Han</a> and <a href="https://research.nvidia.com/person/yao-lu-jason">Yao Lu</a>. My main research interest lies in foundation models for multimodal learning and embodied AI. My long-term research goal is to build a versatile agent to interact with physical world.
              </p>
              <p>
                Previously, I interned at Bytedance working with <a href="https://mingxuan.github.io/">Mingxuan Wang</a>. During my undergraduate, I interned at Mila Qu√©bec, advised by <a href="https://jian-tang.com/">Jian Tang</a>, and Alibaba Damo, advised by <a href="https://sites.google.com/site/chenboxing/">Boxing Chen</a>.
              </p>
              <p>
                I am open to any possible collaboration or discussion, feel free to contact me via email.
              </p>
              <p style="text-align:center">
                <a href="mailto:hujy369@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Cz70Xr8AAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/JamesHujy">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpeg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Education</heading>
              <p>
                B.E.  Department of Computer Science and Technology, <papertitle>Tsinghua University</papertitle>, 2017-2021
                <br>
                <br>
              	Ph.D.  Department of Computer Science and Technology, <papertitle>Tsinghua University</papertitle>, 2021-2026 (expected)
                <br>
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                [2024.6] Open embodied agent platform <a href="https://github.com/thunlp/LEGENT">LEGENT</a> is accepted to ACL 2024 as demo paper.
              </p>
            <p>
              [2024.1] <a href="https://arxiv.org/pdf/2308.12038.pdf">VisCPM</a> is accepted to ICLR 2024 as a spotlight (5%).
            </p>
            <p>
              [2023.6] SOTA Zh-En Multimodal Model <a href="https://github.com/OpenBMB/VisCPM">VisCPM</a> is open-source!
            </p>
            <p>
              [2022.10] Two papers are accepted to EMNLP 2022.
            </p>
            <p>
              [2022.4] <a href="https://arxiv.org/pdf/2207.06130.pdf">Della</a> is accepted to NAACL 2022.
            </p>
            <p>
              [2020.9] One paper is accepted to NeurIPS 2020.
            </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <p>
                (*indicates equal contribution)
              </p>
            </td>
          </tr>
        </tbody>
      </table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <subheading>Embodied AI</subheading>
            </td>
          </tr>
        </tbody>
      </table>
    
    
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    


    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle;">
            <img src='images/LEGENT.webp' width="70%" style="display: block; margin: 0 auto;">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2404.18243.pdf">
          <papertitle>LEGENT: Open Platform for Embodied Agents</papertitle>
        </a>
        <br>
        Zhili Cheng, <strong>Jinyi Hu</strong>, Zhitong Wang,
        <a href="https://shengdinghu.github.io/">Shengding Hu</a>,
        An Liu, Yuge Tu, Pengkai Li, Lei Shi,
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>ACL</em> 2024 demo
        </br>
        [<a href="https://arxiv.org/pdf/2404.18243.pdf">paper</a>]
        [<a href="https://github.com/thunlp/LEGENT">code</a>]
        [<a href="https://docs.legent.ai">doc</a>]
        <p></p>
      </td>
    </tr>
  </tbody>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <subheading>Multimodal Learning</subheading>
            </td>
          </tr>
        </tbody>
      </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
    
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/acdit.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2412.07720">
          <papertitle>ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer</papertitle>
        </a>
        <br>
        <strong>Jinyi Hu*</strong>,
        <a href="https://shengdinghu.github.io/">Shengding Hu*</a>,
        <a href="https://yuxuansong.com/">Yuxuan Song</a>,
        <a href="https://huangyf530.github.io/">Yufei Huang</a>,
        <a href="https://mingxuan.github.io/">Mingxuan Wang</a>,
        Hao Zhou, 
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        Wei-Ying Ma,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>arxiv</em> 2024
        <br>
        [<a href="https://arxiv.org/pdf/2412.07720">paper</a>]
        [<a href="https://github.com/thunlp/ACDiT">code</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
          <img src='images/viscpm.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2308.12038.pdf">
          <papertitle>Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages</papertitle>
        </a>
        <br>
        <strong>Jinyi Hu</strong>,
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>,
        Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang,
        <a href="https://thucsthanxu13.github.io/">Xu Han</a>,
        <a href="https://linyankai.github.io/">Yankai Lin</a>,
        Jiao Xue,
        Dahai Li,
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>ICLR</em> 2024 (<em style="color:red;">spotlight, top 5% </em>)
        <br>
        [<a href="https://arxiv.org/pdf/2308.12038.pdf">paper</a>]
        [<a href="https://github.com/OpenBMB/VisCPM">code (VisCPM)</a>]
        [<a href="https://github.com/OpenBMB/MiniCPM-V">code (MiniCPM-V)</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/rlhfv.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2312.00849.pdf">
          <papertitle>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</papertitle>
        </a>
        <br>
        <a href="https://tianyu-yu.com/">Tianyu Yu</a>,
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>,
        Haoye Zhang, Taiwen He, Yifeng Han,
        <a href="https://cgq15.github.io">Ganqu Cui</a>,
        <strong>Jinyi Hu</strong>,
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.sigs.tsinghua.edu.cn/zht_en/main.htm">Hai-Tao Zheng, </a>
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <a href="https://www.chuatatseng.com/">Tat-Seng Chua</a>
        <br>
        <em>CVPR</em> 2024
        <br>
        [<a href="https://arxiv.org/pdf/2312.00849.pdf">paper</a>]
        [<a href="https://rlhf-v.github.io/">project</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/autonat.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_Revisiting_Non-Autoregressive_Transformers_for_Efficient_Image_Synthesis_CVPR_2024_paper.pdf">
          <papertitle>Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis</papertitle>
        </a>
        <br>
        Zanlin Ni, <a href="https://www.wyl.cool/">Yulin Wang</a>, Renping Zhou, <a href="https://www.jiayiguo.net/">Jiayi Guo</a>,
        <strong>Jinyi Hu</strong>, 
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        Shiji Song,
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>,
        <a href="https://www.gaohuang.net/">Gao Huang</a>
        <br>
        <em>CVPR</em> 2024
        <br>
        [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ni_Revisiting_Non-Autoregressive_Transformers_for_Efficient_Image_Synthesis_CVPR_2024_paper.pdf">paper</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/adanat.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2409.00342">
          <papertitle>AdaNAT: Exploring Adaptive Policy for Token-Based Image Generation</papertitle>
        </a>
        <br>
        Zanlin Ni, <a href="https://www.wyl.cool/">Yulin Wang</a>, Renping Zhou, Rui Lu, <a href="https://www.jiayiguo.net/">Jiayi Guo</a>,
        <strong>Jinyi Hu</strong>, 
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>,
        <a href="https://www.gaohuang.net/">Gao Huang</a>
        <br>
        <em>ECCV</em> 2024
        <br>
        [<a href="https://arxiv.org/pdf/2409.00342">paper</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/guicourse.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2406.11317">
          <papertitle>GUICourse: From General Vision Language Model to Versatile GUI Agent</papertitle>
        </a>
        <br>
        Wentong Chen*, Junbo Cui*, <strong>Jinyi Hu</strong>*, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, 
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>, 
        <a href="https://linyankai.github.io/">Yankai Lin</a>, 
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>arxiv</em> 2024
        <br>
        [<a href="https://arxiv.org/pdf/2406.11317">paper</a>]
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/muffin.png' width="100%">
      </td>
      <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2310.00653.pdf">
          <papertitle>Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants</papertitle>
        </a>
        <br>
        <a href="https://tianyu-yu.com/">Tianyu Yu*</a>,
        <strong>Jinyi Hu*</strong>,
        <a href="https://yaoyuanthu.github.io/">Yuan Yao</a>,
        Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, 
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.sigs.tsinghua.edu.cn/zht_en/main.htm">Hai-Tao Zheng, </a>
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>arxiv</em>, 2023
        <br>
        [<a href="https://arxiv.org/pdf/2310.00653.pdf">paper</a>]
        [<a href="https://github.com/thunlp/muffin">code (Muffin)</a>]
        [<a href="https://huggingface.co/datasets/Yirany/UniMM-Chat">dataset (UniMM-Chat)</a>]
        <p></p>
      </td>
    </tr>
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/iap.png' width="100%">
    </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2305.11540.pdf">
          <papertitle>Efficient Cross-Lingual Transfer for Chinese Stable Diffusion with Images as Pivots</papertitle>
        </a>
        <br>
        <strong>Jinyi Hu</strong>,
        <a href="https://thucsthanxu13.github.io/">Xu Han</a>,
        <a href="https://www.microsoft.com/en-us/research/people/xiaoyuanyi/">Xiaoyuan Yi</a>,
        Yutong Chen,
        Wenhao Li</a>,
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>arxiv</em>, 2023
        <br>
        [<a href="https://arxiv.org/pdf/2305.11540.pdf">paper</a>]
        <p></p>
      </td>
    </tr>


  </tbody>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <subheading>Text Reasoning and Generation</subheading>
        </td>
      </tr>
    </tbody>
  </table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/ob.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2402.14008.pdf">
          <papertitle>OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems</papertitle>
        </a>
        <br>
        Chaoqun He, Renjie Luo, Yuzhuo Bai, <a href="https://shengdinghu.github.io/">Shengding Hu</a>, Zhen Leng Thai, Junhao Shen, <strong>Jinyi Hu</strong>, <a href="https://thucsthanxu13.github.io/">Xu Han</a>, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi,
        <a href="https://nlp.csai.tsinghua.edu.cn/~lzy/">Zhiyuan Liu</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
        <br>
        <em>ACL</em> 2024
        <br>
        [<a href="https://arxiv.org/pdf/2402.14008.pdf">paper</a>]
        [<a href="https://github.com/OpenBMB/OlympiadBench">code</a>]
        <p></p>
      </td>
    </tr>


    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/della.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
        <a href="https://arxiv.org/pdf/2207.06130.pdf">
          <papertitle>Fuse It More Deeply! A Variational Transformer with Layer-Wise Latent Variable Inference for Text Generation</papertitle>
        </a>
        <br>
        <strong>Jinyi Hu</strong>,
        <a href="https://www.microsoft.com/en-us/research/people/xiaoyuanyi/">Xiaoyuan Yi</a>,
        Wenhao Li</a>,
        <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>,
        Xing Xie
        <br>
        <em>NAACL</em> 2022 (<em style="color:red;">oral</em>)
        <br>
        [<a href="https://arxiv.org/pdf/2207.06130.pdf">paper</a>]
        [<a href="https://github.com/OpenVLG/DELLA">code (DELLA)</a>]
        <p></p>
      </td>
    </tr>
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/trace.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2210.12409.pdf">
        <papertitle>Recurrence Boosts Diversity! Revisiting Recurrent Latent Variable in Transformer-Based Variational AutoEncoder for Diverse Text Generation</papertitle>
      </a>
      <br>
      <strong>Jinyi Hu</strong>,
      <a href="https://www.microsoft.com/en-us/research/people/xiaoyuanyi/">Xiaoyuan Yi</a>,
      Wenhao Li</a>,
      <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>,
      Xing Xie
      <br>
      <em>Findings of EMNLP</em> 2022
      <br>
      [<a href="https://arxiv.org/pdf/2210.12409.pdf">paper</a>]
      <p></p>
    </td>
    </tr>
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/elv.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
      <a href="https://proceedings.neurips.cc/paper/2020/file/4be2c8f27b8a420492f2d44463933eb6-Paper.pdf">
        <papertitle>Towards interpretable natural language understanding with explanations as latent variables</papertitle>
      </a>
      <br>
      <strong>Jinyi Hu*</strong>,
      <a href="https://michaelzhouwang.github.io/">Wangchunshu Zhou*</a>,
      <a href="https://hanlin-zhang.com/">Hanlin Zhang*</a>,
      <a href="https://lemondan.github.io/">Xiaodan Liang</a>,
      <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>,
      <a href="https://www.microsoft.com/en-us/research/people/cxiong/">Chenyan Xiong</a>,
      <a href="https://jian-tang.com/">Jian Tang</a>,
      <br>
      <em>NeurIPS</em> 2020, <strong>cite by <a href="https://arxiv.org/pdf/2201.11903">CoT</a>, <a href="https://arxiv.org/pdf/2203.14465">STaR</a></strong>
      <br>
      [<a href="https://proceedings.neurips.cc/paper/2020/file/4be2c8f27b8a420492f2d44463933eb6-Paper.pdf">paper</a>]
      [<a href="https://github.com/JamesHujy/ELV">code</a>]
      <p></p>
    </td>
    </tr>

    
    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/care.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
      <a href="https://arxiv.org/pdf/2211.07164.pdf">
        <papertitle>Evade the Trap of Mediocrity: Promoting Diversity and Novelty in Text Generation via Concentrating Attention</papertitle>
      </a>
      <br>
      Wenhao Li</a>,
      <a href="https://www.microsoft.com/en-us/research/people/xiaoyuanyi/">Xiaoyuan Yi</a>,
      <strong>Jinyi Hu</strong>,
      <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>,
      Xing Xie
      <br>
      <em>EMNLP</em> 2022
      <br>
      [<a href="https://arxiv.org/pdf/2211.07164.pdf">paper</a>]
      <p></p>
    </td>
    </tr>

    

    <tr>
      <td style="padding:10px;width:30%;vertical-align:middle">
        <img src='images/asrg.png' width="100%">
      </td>
    <td style="padding:10px;width:70%;vertical-align:middle">
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/17497/17304">
        <papertitle>Aspect-level sentiment-controllable review generation with mutual learning framework</papertitle>
      </a>
      <br>
      <a href="http://nlp.csai.tsinghua.edu.cn/~chm/">Huimin Chen</a>,
      <a href="https://linyankai.github.io/">Yankai Lin</a>,
      <a href="https://fanchao-qi.github.io/">Fanchao Qi</a>,
      <strong>Jinyi Hu</strong>,
      <a href="https://www.lpeng.net/">Peng Li</a>,
      Jie Zhou,
      <a href="https://www.cs.tsinghua.edu.cn/csen/info/1180/4033.htm">Maosong Sun</a>
      <br>
      <em>AAAI</em> 2021
      <br>
      [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/17497/17304">paper</a>]
      <p></p>
    </td>
    </tr>  
    </tr>
  </tbody>
  </table>
        
        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>
                  <p>
                    Reviewer: NeurIPS, ICLR, CVPR, ACL, EMNLP, NAACL
                  </p>
                </td>
              </tr>
            </tbody>
        </table>

        <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Honors & Awards</heading>
                  <ul>
                    <li>Comprehensive Scholarship, 2023</li>
                    <li>Longhu Scholarship, 2023</li>
                    <li>Comprehensive Scholarship, 2022</li>
                    <li>Merit Student in Beijing, 2020</li>
                    <li>Tang Lixin Scholarship, 2020</li>
                    <li>Evergrande Scholarship, 2018</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

				<table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Miscellaneous</heading>
                  <p>
                    I usually go to the gym during the spare time. I also love NBA, Snooker and Tennis, a big fan of LeBron James, Ronnie O'Sullivan and Roger Federer. I love the number 7.
                  </p>
                </td>
              </tr>
            </tbody>
        </table>
					
					
        </tbody></table>
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
              </p>
            </td>
          </tr>
        </tbody></table> -->
      </td>
    </tr>
  </table>
</body>

</html>
